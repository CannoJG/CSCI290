{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebcc74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a665850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT_Module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c00f51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"torch\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9621c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bc887d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"gpt2-small-124M.pth\"\n",
    "# file_name = \"gpt2-medium-355M.pth\"\n",
    "# file_name = \"gpt2-large-774M.pth\"\n",
    "# file_name = \"gpt2-xl-1558M.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e478a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    response = requests.get(url, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3451c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPTModel:\n\tMissing key(s) in state_dict: \"trf_blocks.0.feedforward.layers.0.weight\", \"trf_blocks.0.feedforward.layers.0.bias\", \"trf_blocks.0.feedforward.layers.2.weight\", \"trf_blocks.0.feedforward.layers.2.bias\", \"trf_blocks.1.feedforward.layers.0.weight\", \"trf_blocks.1.feedforward.layers.0.bias\", \"trf_blocks.1.feedforward.layers.2.weight\", \"trf_blocks.1.feedforward.layers.2.bias\", \"trf_blocks.2.feedforward.layers.0.weight\", \"trf_blocks.2.feedforward.layers.0.bias\", \"trf_blocks.2.feedforward.layers.2.weight\", \"trf_blocks.2.feedforward.layers.2.bias\", \"trf_blocks.3.feedforward.layers.0.weight\", \"trf_blocks.3.feedforward.layers.0.bias\", \"trf_blocks.3.feedforward.layers.2.weight\", \"trf_blocks.3.feedforward.layers.2.bias\", \"trf_blocks.4.feedforward.layers.0.weight\", \"trf_blocks.4.feedforward.layers.0.bias\", \"trf_blocks.4.feedforward.layers.2.weight\", \"trf_blocks.4.feedforward.layers.2.bias\", \"trf_blocks.5.feedforward.layers.0.weight\", \"trf_blocks.5.feedforward.layers.0.bias\", \"trf_blocks.5.feedforward.layers.2.weight\", \"trf_blocks.5.feedforward.layers.2.bias\", \"trf_blocks.6.feedforward.layers.0.weight\", \"trf_blocks.6.feedforward.layers.0.bias\", \"trf_blocks.6.feedforward.layers.2.weight\", \"trf_blocks.6.feedforward.layers.2.bias\", \"trf_blocks.7.feedforward.layers.0.weight\", \"trf_blocks.7.feedforward.layers.0.bias\", \"trf_blocks.7.feedforward.layers.2.weight\", \"trf_blocks.7.feedforward.layers.2.bias\", \"trf_blocks.8.feedforward.layers.0.weight\", \"trf_blocks.8.feedforward.layers.0.bias\", \"trf_blocks.8.feedforward.layers.2.weight\", \"trf_blocks.8.feedforward.layers.2.bias\", \"trf_blocks.9.feedforward.layers.0.weight\", \"trf_blocks.9.feedforward.layers.0.bias\", \"trf_blocks.9.feedforward.layers.2.weight\", \"trf_blocks.9.feedforward.layers.2.bias\", \"trf_blocks.10.feedforward.layers.0.weight\", \"trf_blocks.10.feedforward.layers.0.bias\", \"trf_blocks.10.feedforward.layers.2.weight\", \"trf_blocks.10.feedforward.layers.2.bias\", \"trf_blocks.11.feedforward.layers.0.weight\", \"trf_blocks.11.feedforward.layers.0.bias\", \"trf_blocks.11.feedforward.layers.2.weight\", \"trf_blocks.11.feedforward.layers.2.bias\". \n\tUnexpected key(s) in state_dict: \"trf_blocks.0.ff.layers.0.weight\", \"trf_blocks.0.ff.layers.0.bias\", \"trf_blocks.0.ff.layers.2.weight\", \"trf_blocks.0.ff.layers.2.bias\", \"trf_blocks.0.att.W_query.bias\", \"trf_blocks.0.att.W_key.bias\", \"trf_blocks.0.att.W_value.bias\", \"trf_blocks.1.ff.layers.0.weight\", \"trf_blocks.1.ff.layers.0.bias\", \"trf_blocks.1.ff.layers.2.weight\", \"trf_blocks.1.ff.layers.2.bias\", \"trf_blocks.1.att.W_query.bias\", \"trf_blocks.1.att.W_key.bias\", \"trf_blocks.1.att.W_value.bias\", \"trf_blocks.2.ff.layers.0.weight\", \"trf_blocks.2.ff.layers.0.bias\", \"trf_blocks.2.ff.layers.2.weight\", \"trf_blocks.2.ff.layers.2.bias\", \"trf_blocks.2.att.W_query.bias\", \"trf_blocks.2.att.W_key.bias\", \"trf_blocks.2.att.W_value.bias\", \"trf_blocks.3.ff.layers.0.weight\", \"trf_blocks.3.ff.layers.0.bias\", \"trf_blocks.3.ff.layers.2.weight\", \"trf_blocks.3.ff.layers.2.bias\", \"trf_blocks.3.att.W_query.bias\", \"trf_blocks.3.att.W_key.bias\", \"trf_blocks.3.att.W_value.bias\", \"trf_blocks.4.ff.layers.0.weight\", \"trf_blocks.4.ff.layers.0.bias\", \"trf_blocks.4.ff.layers.2.weight\", \"trf_blocks.4.ff.layers.2.bias\", \"trf_blocks.4.att.W_query.bias\", \"trf_blocks.4.att.W_key.bias\", \"trf_blocks.4.att.W_value.bias\", \"trf_blocks.5.ff.layers.0.weight\", \"trf_blocks.5.ff.layers.0.bias\", \"trf_blocks.5.ff.layers.2.weight\", \"trf_blocks.5.ff.layers.2.bias\", \"trf_blocks.5.att.W_query.bias\", \"trf_blocks.5.att.W_key.bias\", \"trf_blocks.5.att.W_value.bias\", \"trf_blocks.6.ff.layers.0.weight\", \"trf_blocks.6.ff.layers.0.bias\", \"trf_blocks.6.ff.layers.2.weight\", \"trf_blocks.6.ff.layers.2.bias\", \"trf_blocks.6.att.W_query.bias\", \"trf_blocks.6.att.W_key.bias\", \"trf_blocks.6.att.W_value.bias\", \"trf_blocks.7.ff.layers.0.weight\", \"trf_blocks.7.ff.layers.0.bias\", \"trf_blocks.7.ff.layers.2.weight\", \"trf_blocks.7.ff.layers.2.bias\", \"trf_blocks.7.att.W_query.bias\", \"trf_blocks.7.att.W_key.bias\", \"trf_blocks.7.att.W_value.bias\", \"trf_blocks.8.ff.layers.0.weight\", \"trf_blocks.8.ff.layers.0.bias\", \"trf_blocks.8.ff.layers.2.weight\", \"trf_blocks.8.ff.layers.2.bias\", \"trf_blocks.8.att.W_query.bias\", \"trf_blocks.8.att.W_key.bias\", \"trf_blocks.8.att.W_value.bias\", \"trf_blocks.9.ff.layers.0.weight\", \"trf_blocks.9.ff.layers.0.bias\", \"trf_blocks.9.ff.layers.2.weight\", \"trf_blocks.9.ff.layers.2.bias\", \"trf_blocks.9.att.W_query.bias\", \"trf_blocks.9.att.W_key.bias\", \"trf_blocks.9.att.W_value.bias\", \"trf_blocks.10.ff.layers.0.weight\", \"trf_blocks.10.ff.layers.0.bias\", \"trf_blocks.10.ff.layers.2.weight\", \"trf_blocks.10.ff.layers.2.bias\", \"trf_blocks.10.att.W_query.bias\", \"trf_blocks.10.att.W_key.bias\", \"trf_blocks.10.att.W_value.bias\", \"trf_blocks.11.ff.layers.0.weight\", \"trf_blocks.11.ff.layers.0.bias\", \"trf_blocks.11.ff.layers.2.weight\", \"trf_blocks.11.ff.layers.2.bias\", \"trf_blocks.11.att.W_query.bias\", \"trf_blocks.11.att.W_key.bias\", \"trf_blocks.11.att.W_value.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# from llms_from_scratch.ch04 import GPTModel\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# For llms_from_scratch installation instructions, see:\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\u001b[39;00m\n\u001b[32m      6\u001b[39m gpt = GPTModel(BASE_CONFIG)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mgpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m gpt.eval()\n\u001b[32m     10\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for GPTModel:\n\tMissing key(s) in state_dict: \"trf_blocks.0.feedforward.layers.0.weight\", \"trf_blocks.0.feedforward.layers.0.bias\", \"trf_blocks.0.feedforward.layers.2.weight\", \"trf_blocks.0.feedforward.layers.2.bias\", \"trf_blocks.1.feedforward.layers.0.weight\", \"trf_blocks.1.feedforward.layers.0.bias\", \"trf_blocks.1.feedforward.layers.2.weight\", \"trf_blocks.1.feedforward.layers.2.bias\", \"trf_blocks.2.feedforward.layers.0.weight\", \"trf_blocks.2.feedforward.layers.0.bias\", \"trf_blocks.2.feedforward.layers.2.weight\", \"trf_blocks.2.feedforward.layers.2.bias\", \"trf_blocks.3.feedforward.layers.0.weight\", \"trf_blocks.3.feedforward.layers.0.bias\", \"trf_blocks.3.feedforward.layers.2.weight\", \"trf_blocks.3.feedforward.layers.2.bias\", \"trf_blocks.4.feedforward.layers.0.weight\", \"trf_blocks.4.feedforward.layers.0.bias\", \"trf_blocks.4.feedforward.layers.2.weight\", \"trf_blocks.4.feedforward.layers.2.bias\", \"trf_blocks.5.feedforward.layers.0.weight\", \"trf_blocks.5.feedforward.layers.0.bias\", \"trf_blocks.5.feedforward.layers.2.weight\", \"trf_blocks.5.feedforward.layers.2.bias\", \"trf_blocks.6.feedforward.layers.0.weight\", \"trf_blocks.6.feedforward.layers.0.bias\", \"trf_blocks.6.feedforward.layers.2.weight\", \"trf_blocks.6.feedforward.layers.2.bias\", \"trf_blocks.7.feedforward.layers.0.weight\", \"trf_blocks.7.feedforward.layers.0.bias\", \"trf_blocks.7.feedforward.layers.2.weight\", \"trf_blocks.7.feedforward.layers.2.bias\", \"trf_blocks.8.feedforward.layers.0.weight\", \"trf_blocks.8.feedforward.layers.0.bias\", \"trf_blocks.8.feedforward.layers.2.weight\", \"trf_blocks.8.feedforward.layers.2.bias\", \"trf_blocks.9.feedforward.layers.0.weight\", \"trf_blocks.9.feedforward.layers.0.bias\", \"trf_blocks.9.feedforward.layers.2.weight\", \"trf_blocks.9.feedforward.layers.2.bias\", \"trf_blocks.10.feedforward.layers.0.weight\", \"trf_blocks.10.feedforward.layers.0.bias\", \"trf_blocks.10.feedforward.layers.2.weight\", \"trf_blocks.10.feedforward.layers.2.bias\", \"trf_blocks.11.feedforward.layers.0.weight\", \"trf_blocks.11.feedforward.layers.0.bias\", \"trf_blocks.11.feedforward.layers.2.weight\", \"trf_blocks.11.feedforward.layers.2.bias\". \n\tUnexpected key(s) in state_dict: \"trf_blocks.0.ff.layers.0.weight\", \"trf_blocks.0.ff.layers.0.bias\", \"trf_blocks.0.ff.layers.2.weight\", \"trf_blocks.0.ff.layers.2.bias\", \"trf_blocks.0.att.W_query.bias\", \"trf_blocks.0.att.W_key.bias\", \"trf_blocks.0.att.W_value.bias\", \"trf_blocks.1.ff.layers.0.weight\", \"trf_blocks.1.ff.layers.0.bias\", \"trf_blocks.1.ff.layers.2.weight\", \"trf_blocks.1.ff.layers.2.bias\", \"trf_blocks.1.att.W_query.bias\", \"trf_blocks.1.att.W_key.bias\", \"trf_blocks.1.att.W_value.bias\", \"trf_blocks.2.ff.layers.0.weight\", \"trf_blocks.2.ff.layers.0.bias\", \"trf_blocks.2.ff.layers.2.weight\", \"trf_blocks.2.ff.layers.2.bias\", \"trf_blocks.2.att.W_query.bias\", \"trf_blocks.2.att.W_key.bias\", \"trf_blocks.2.att.W_value.bias\", \"trf_blocks.3.ff.layers.0.weight\", \"trf_blocks.3.ff.layers.0.bias\", \"trf_blocks.3.ff.layers.2.weight\", \"trf_blocks.3.ff.layers.2.bias\", \"trf_blocks.3.att.W_query.bias\", \"trf_blocks.3.att.W_key.bias\", \"trf_blocks.3.att.W_value.bias\", \"trf_blocks.4.ff.layers.0.weight\", \"trf_blocks.4.ff.layers.0.bias\", \"trf_blocks.4.ff.layers.2.weight\", \"trf_blocks.4.ff.layers.2.bias\", \"trf_blocks.4.att.W_query.bias\", \"trf_blocks.4.att.W_key.bias\", \"trf_blocks.4.att.W_value.bias\", \"trf_blocks.5.ff.layers.0.weight\", \"trf_blocks.5.ff.layers.0.bias\", \"trf_blocks.5.ff.layers.2.weight\", \"trf_blocks.5.ff.layers.2.bias\", \"trf_blocks.5.att.W_query.bias\", \"trf_blocks.5.att.W_key.bias\", \"trf_blocks.5.att.W_value.bias\", \"trf_blocks.6.ff.layers.0.weight\", \"trf_blocks.6.ff.layers.0.bias\", \"trf_blocks.6.ff.layers.2.weight\", \"trf_blocks.6.ff.layers.2.bias\", \"trf_blocks.6.att.W_query.bias\", \"trf_blocks.6.att.W_key.bias\", \"trf_blocks.6.att.W_value.bias\", \"trf_blocks.7.ff.layers.0.weight\", \"trf_blocks.7.ff.layers.0.bias\", \"trf_blocks.7.ff.layers.2.weight\", \"trf_blocks.7.ff.layers.2.bias\", \"trf_blocks.7.att.W_query.bias\", \"trf_blocks.7.att.W_key.bias\", \"trf_blocks.7.att.W_value.bias\", \"trf_blocks.8.ff.layers.0.weight\", \"trf_blocks.8.ff.layers.0.bias\", \"trf_blocks.8.ff.layers.2.weight\", \"trf_blocks.8.ff.layers.2.bias\", \"trf_blocks.8.att.W_query.bias\", \"trf_blocks.8.att.W_key.bias\", \"trf_blocks.8.att.W_value.bias\", \"trf_blocks.9.ff.layers.0.weight\", \"trf_blocks.9.ff.layers.0.bias\", \"trf_blocks.9.ff.layers.2.weight\", \"trf_blocks.9.ff.layers.2.bias\", \"trf_blocks.9.att.W_query.bias\", \"trf_blocks.9.att.W_key.bias\", \"trf_blocks.9.att.W_value.bias\", \"trf_blocks.10.ff.layers.0.weight\", \"trf_blocks.10.ff.layers.0.bias\", \"trf_blocks.10.ff.layers.2.weight\", \"trf_blocks.10.ff.layers.2.bias\", \"trf_blocks.10.att.W_query.bias\", \"trf_blocks.10.att.W_key.bias\", \"trf_blocks.10.att.W_value.bias\", \"trf_blocks.11.ff.layers.0.weight\", \"trf_blocks.11.ff.layers.0.bias\", \"trf_blocks.11.ff.layers.2.weight\", \"trf_blocks.11.ff.layers.2.bias\", \"trf_blocks.11.att.W_query.bias\", \"trf_blocks.11.att.W_key.bias\", \"trf_blocks.11.att.W_value.bias\". "
     ]
    }
   ],
   "source": [
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# For llms_from_scratch installation instructions, see:\n",
    "# https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74db656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_from_scratch.ch05 import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    response = requests.get(url, stream=True, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with open(zip_path, \"wb\") as out_file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                out_file.write(chunk)\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (requests.exceptions.RequestException, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5beb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96221255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    \n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9323e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "        # Note: A more pythonic version to implement this method\n",
    "        # is the following, which is also used in the next chapter:\n",
    "        # return max(len(encoded_text) for encoded_text in self.encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f238f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40327226",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e962690",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc577870",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10300b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f1c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3122e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a631f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in chapter 5\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28909e0",
   "metadata": {},
   "source": [
    "## Actual Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall the same as `train_model_simple` in chapter 5\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as chapter 5\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741604ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
    "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    assert max_length is not None, (\n",
    "        \"max_length must be specified. If you want to use the full model context, \"\n",
    "        \"pass max_length=model.pos_emb.weight.shape[0].\"\n",
    "    )\n",
    "    assert max_length <= supported_context_length, (\n",
    "        f\"max_length ({max_length}) exceeds model's supported context length ({supported_context_length}).\"\n",
    "    )    \n",
    "    # Alternatively, a more robust version is the following one, which handles the max_length=None case better\n",
    "    # max_len = min(max_length,supported_context_length) if max_length else supported_context_length\n",
    "    # input_ids = input_ids[:max_len]\n",
    "    \n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29044fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce653c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
