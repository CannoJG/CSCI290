{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a69834",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "b491987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5cbc8",
   "metadata": {},
   "source": [
    "### Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "0b85517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches by stacking 2 input vectors together\n",
    "inputs = torch.nn.Embedding(6,3)\n",
    "inputs = inputs.weight.data\n",
    "batches = torch.stack((inputs, inputs), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "e50f2858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3406,  2.0035, -0.4027],\n",
      "        [ 0.4818,  0.3113,  0.0386],\n",
      "        [-0.7591, -0.7128, -0.5769],\n",
      "        [ 0.7443, -0.1105, -0.0138],\n",
      "        [ 0.7773,  0.2322, -1.0294],\n",
      "        [ 0.7965,  0.8256,  0.8181]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b66ab20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3406,  2.0035, -0.4027],\n",
       "         [ 0.4818,  0.3113,  0.0386],\n",
       "         [-0.7591, -0.7128, -0.5769],\n",
       "         [ 0.7443, -0.1105, -0.0138],\n",
       "         [ 0.7773,  0.2322, -1.0294],\n",
       "         [ 0.7965,  0.8256,  0.8181]],\n",
       "\n",
       "        [[-0.3406,  2.0035, -0.4027],\n",
       "         [ 0.4818,  0.3113,  0.0386],\n",
       "         [-0.7591, -0.7128, -0.5769],\n",
       "         [ 0.7443, -0.1105, -0.0138],\n",
       "         [ 0.7773,  0.2322, -1.0294],\n",
       "         [ 0.7965,  0.8256,  0.8181]]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "c00cd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain view methods and transpositions in the multiheadattention class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ca9a254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multihead Attention Class for reference \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "ffe1432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a reference class\n",
    "mha = MultiHeadAttention(d_in=3, d_out= 2, context_length= 6, dropout= 0, num_heads= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "c2ce9ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "#Find context vectors from sample ID vector batches\n",
    "mha_out = mha(batches)\n",
    "mha_out\n",
    "print(mha_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "791dc99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch_size, context_length, d_in = batches.shape\n",
    "b, num_tokens, d_in = batches.shape\n",
    "d_out = 2\n",
    "#Shape of the bactch. Thus b = 2, num = 4, d_in = 3\n",
    "#Decide number of attention heads...\n",
    "num_heads = 2\n",
    "qkv_bias = False\n",
    "dropout = 0\n",
    "#Calculate head dimensions\n",
    "head_dim = d_out // num_heads\n",
    "print(d_in)\n",
    "print(batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "5f8b5fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key weights: Linear(in_features=3, out_features=2, bias=False)\n",
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "# Creat query, key, value matrices of old vector length (2) by new vector length (3)\n",
    "\n",
    "W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "dropout = nn.Dropout(dropout)\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "\n",
    "# Weight matrices are 3 x 2\n",
    "print(\"key weights:\", W_key)\n",
    "# Mask matrix is 6 x 6 \n",
    "print(mask)\n",
    "print(mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "ff54e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False) X torch.Size([2, 6, 3])\n",
      "resulting tensor: torch.Size([2, 6, 2])\n",
      "Queries: tensor([[[ 0.1446,  0.8069],\n",
      "         [-0.2327, -0.1224],\n",
      "         [ 0.0879, -0.0064],\n",
      "         [-0.4420, -0.4205],\n",
      "         [-0.9495, -0.5462],\n",
      "         [ 0.0216,  0.0837]],\n",
      "\n",
      "        [[ 0.1446,  0.8069],\n",
      "         [-0.2327, -0.1224],\n",
      "         [ 0.0879, -0.0064],\n",
      "         [-0.4420, -0.4205],\n",
      "         [-0.9495, -0.5462],\n",
      "         [ 0.0216,  0.0837]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Creates 3 matrices of correct dimensions by multiplying random weight matrix (3 x 2) by input matrix (2 x 6 x 3))\n",
    "keys = W_key(batches) # Shape: (b, num_tokens, d_out)\n",
    "queries = W_query(batches)\n",
    "values = W_value(batches)\n",
    "print(W_key, \"X\", batches.shape)\n",
    "\n",
    "print(\"resulting tensor:\", keys.shape)\n",
    "print(\"Queries:\", keys)\n",
    "\n",
    "# Returns matrices of 2 (b) x 6 (number tokens) x 2 (d_out) = 2 x 6 x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3dd7ddba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2, 1])\n",
      "Keys: tensor([[[[ 0.1446],\n",
      "          [ 0.8069]],\n",
      "\n",
      "         [[-0.2327],\n",
      "          [-0.1224]],\n",
      "\n",
      "         [[ 0.0879],\n",
      "          [-0.0064]],\n",
      "\n",
      "         [[-0.4420],\n",
      "          [-0.4205]],\n",
      "\n",
      "         [[-0.9495],\n",
      "          [-0.5462]],\n",
      "\n",
      "         [[ 0.0216],\n",
      "          [ 0.0837]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1446],\n",
      "          [ 0.8069]],\n",
      "\n",
      "         [[-0.2327],\n",
      "          [-0.1224]],\n",
      "\n",
      "         [[ 0.0879],\n",
      "          [-0.0064]],\n",
      "\n",
      "         [[-0.4420],\n",
      "          [-0.4205]],\n",
      "\n",
      "         [[-0.9495],\n",
      "          [-0.5462]],\n",
      "\n",
      "         [[ 0.0216],\n",
      "          [ 0.0837]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#View method converts to dimensions to 2 x 6 x 2 x 1\n",
    "\n",
    "keys = keys.view(b, num_tokens, num_heads, head_dim) \n",
    "values = values.view(b, num_tokens, num_heads, head_dim)\n",
    "queries = queries.view(b, num_tokens, num_heads, head_dim)\n",
    "print(keys.shape)\n",
    "print(\"Keys:\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "da2c1006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[[[ 0.1446],\n",
      "          [-0.2327],\n",
      "          [ 0.0879],\n",
      "          [-0.4420],\n",
      "          [-0.9495],\n",
      "          [ 0.0216]],\n",
      "\n",
      "         [[ 0.8069],\n",
      "          [-0.1224],\n",
      "          [-0.0064],\n",
      "          [-0.4205],\n",
      "          [-0.5462],\n",
      "          [ 0.0837]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1446],\n",
      "          [-0.2327],\n",
      "          [ 0.0879],\n",
      "          [-0.4420],\n",
      "          [-0.9495],\n",
      "          [ 0.0216]],\n",
      "\n",
      "         [[ 0.8069],\n",
      "          [-0.1224],\n",
      "          [-0.0064],\n",
      "          [-0.4205],\n",
      "          [-0.5462],\n",
      "          [ 0.0837]]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 2, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "#transpose function transposes matrices on 2nd and 3rd dimensions so 2 x 6 x 2 x 1 becomes 2 x 2 x 6 x 1 \n",
    "\n",
    "keys = keys.transpose(1, 2)\n",
    "queries = queries.transpose(1, 2)\n",
    "values = values.transpose(1, 2)\n",
    "print(\"Keys:\", keys)\n",
    "print(keys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0527,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0153,  0.0246,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0197, -0.0316,  0.0120,    -inf,    -inf,    -inf],\n",
      "          [-0.0101,  0.0163, -0.0062,  0.0309,    -inf,    -inf],\n",
      "          [-0.0391,  0.0629, -0.0238,  0.1196,  0.2569,    -inf],\n",
      "          [-0.0185,  0.0297, -0.0112,  0.0564,  0.1212, -0.0028]],\n",
      "\n",
      "         [[-0.2159,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1774,  0.0269,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2699, -0.0409, -0.0021,    -inf,    -inf,    -inf],\n",
      "          [-0.1970,  0.0299,  0.0016,  0.1027,    -inf,    -inf],\n",
      "          [-0.3347,  0.0508,  0.0026,  0.1744,  0.2266,    -inf],\n",
      "          [-0.2773,  0.0421,  0.0022,  0.1445,  0.1877, -0.0288]]],\n",
      "\n",
      "\n",
      "        [[[-0.0527,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0153,  0.0246,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0197, -0.0316,  0.0120,    -inf,    -inf,    -inf],\n",
      "          [-0.0101,  0.0163, -0.0062,  0.0309,    -inf,    -inf],\n",
      "          [-0.0391,  0.0629, -0.0238,  0.1196,  0.2569,    -inf],\n",
      "          [-0.0185,  0.0297, -0.0112,  0.0564,  0.1212, -0.0028]],\n",
      "\n",
      "         [[-0.2159,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1774,  0.0269,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2699, -0.0409, -0.0021,    -inf,    -inf,    -inf],\n",
      "          [-0.1970,  0.0299,  0.0016,  0.1027,    -inf,    -inf],\n",
      "          [-0.3347,  0.0508,  0.0026,  0.1744,  0.2266,    -inf],\n",
      "          [-0.2773,  0.0421,  0.0022,  0.1445,  0.1877, -0.0288]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#multiply queries (2x2x6x1) by keys transposed on 3rd and 4th dimension(2x2x1x6) \n",
    "attn_scores = queries @ keys.transpose(2, 3)\n",
    "mask_bool = mask.bool()[:num_tokens, :num_tokens]\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "#Multiplication returns a 2x2x6x6\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6a98d32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4900, 0.5100, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3399, 0.3229, 0.3373, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2455, 0.2521, 0.2465, 0.2558, 0.0000, 0.0000],\n",
      "          [0.1773, 0.1964, 0.1801, 0.2078, 0.2384, 0.0000],\n",
      "          [0.1587, 0.1666, 0.1599, 0.1711, 0.1825, 0.1612]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4491, 0.5509, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4009, 0.2938, 0.3054, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2073, 0.2601, 0.2528, 0.2797, 0.0000, 0.0000],\n",
      "          [0.1372, 0.2017, 0.1923, 0.2283, 0.2405, 0.0000],\n",
      "          [0.1235, 0.1700, 0.1633, 0.1883, 0.1966, 0.1583]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4900, 0.5100, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3399, 0.3229, 0.3373, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2455, 0.2521, 0.2465, 0.2558, 0.0000, 0.0000],\n",
      "          [0.1773, 0.1964, 0.1801, 0.2078, 0.2384, 0.0000],\n",
      "          [0.1587, 0.1666, 0.1599, 0.1711, 0.1825, 0.1612]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4491, 0.5509, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4009, 0.2938, 0.3054, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2073, 0.2601, 0.2528, 0.2797, 0.0000, 0.0000],\n",
      "          [0.1372, 0.2017, 0.1923, 0.2283, 0.2405, 0.0000],\n",
      "          [0.1235, 0.1700, 0.1633, 0.1883, 0.1966, 0.1583]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 2, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights = dropout(attn_weights)\n",
    "print(attn_weights)\n",
    "print (attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "9332ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 6, 6])\n",
      "torch.Size([2, 6, 2, 1])\n",
      "equals:\n",
      "torch.Size([2, 6, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Multiply attention weights (2x2x6x6) with values transposed on the 2nd and 3rd dimensions (2x6x2x1)\n",
    "\n",
    "print(attn_weights.shape)\n",
    "print(values.transpose(1,2).shape)\n",
    "print(\"equals:\")\n",
    "context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "print(context_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "047eb965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n",
      "tensor([[[-0.6197, -0.6902],\n",
      "         [-0.3749, -0.4036],\n",
      "         [-0.1447, -0.2254],\n",
      "         [-0.1095, -0.1181],\n",
      "         [-0.0953, -0.1280],\n",
      "         [-0.1446, -0.1698]],\n",
      "\n",
      "        [[-0.6197, -0.6902],\n",
      "         [-0.3749, -0.4036],\n",
      "         [-0.1447, -0.2254],\n",
      "         [-0.1095, -0.1181],\n",
      "         [-0.0953, -0.1280],\n",
      "         [-0.1446, -0.1698]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Contiguous converts 2x6x2x1 into 2x6x2 \n",
    "context_vec = context_vec.contiguous().view(\n",
    "         b, num_tokens, d_out\n",
    ")\n",
    "print(context_vec.shape)\n",
    "print(context_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
